# TSDB Design Document

## Table of Contents

1. [Overview](#overview)
2. [Design Goals](#design-goals)
3. [Architecture](#architecture)
4. [Phase 1: Core Data Structures](#phase-1-core-data-structures)
5. [Design Decisions](#design-decisions)
6. [Performance Considerations](#performance-considerations)
7. [Future Phases](#future-phases)

## Overview

This document describes the architecture and design decisions for the Time-Series Database (TSDB) project. The database is designed from scratch in Go to achieve high performance, efficient storage, and production-grade reliability.

## Design Goals

### Primary Goals

1. **High Write Throughput**: 100K-500K samples/second on commodity hardware
2. **Fast Queries**: Sub-100ms query latency for typical workloads
3. **Efficient Storage**: 10-20x compression ratio using Gorilla paper algorithms
4. **Memory Efficiency**: Support 1M+ active series with <512MB RAM
5. **Durability**: Zero data loss with write-ahead logging
6. **Scalability**: Handle high-cardinality label sets

### Non-Goals (for now)

- Distributed/clustered operation (single-node first)
- Built-in alerting (can be built on top)
- Schema migrations (immutable schema)

## Architecture

### High-Level Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Write Path                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Client â†’ API â†’ WAL â†’ MemTable â†’ Flush â†’ Disk Blocks      â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        Read Path                            â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                             â”‚
â”‚  Client â†’ API â†’ Query Engine â†’ Index â†’ MemTable + Blocks   â”‚
â”‚                         â†“                                    â”‚
â”‚                   Decompression                              â”‚
â”‚                         â†“                                    â”‚
â”‚                   Result Merging                             â”‚
â”‚                                                             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Phase 1: Core Data Structures

### Sample

The `Sample` is the fundamental unit of time-series data:

```go
type Sample struct {
    Timestamp int64   // Unix milliseconds
    Value     float64 // Metric value
}
```

**Design Decisions:**

- **Timestamp as int64**: Unix milliseconds provide nanosecond precision isn't needed for most monitoring use cases, and 64-bit integers are efficient
- **Value as float64**: Standard IEEE 754 floating-point for numeric metrics
- **Size**: 16 bytes per sample in memory (8+8)

### Series

A `Series` represents a unique time-series identified by labels:

```go
type Series struct {
    Labels map[string]string // Label key-value pairs
    Hash   uint64            // Computed hash for fast lookup
}
```

**Design Decisions:**

- **Label-based identification**: Follows Prometheus model, very flexible
- **Hash caching**: Computing hash once at creation time for O(1) lookups
- **FNV-1a hash algorithm**: Good distribution, fast computation, no crypto needed

**Label Structure:**

- Special label `__name__` identifies the metric name
- Additional labels provide dimensions (host, region, etc.)
- Example: `{__name__="cpu_usage", host="server1", region="us-west"}`

### Series Hashing

Series hashing is critical for performance. Our implementation:

1. **Deterministic**: Same labels always produce same hash
2. **Order-independent**: Label insertion order doesn't affect hash
3. **Fast**: Can hash 1M+ series per second
4. **Collision-resistant**: Uses FNV-1a for good distribution

**Algorithm:**

```go
func (s *Series) computeHash() uint64 {
    // 1. Sort label names for consistency
    names := sortedKeys(s.Labels)

    // 2. Hash using FNV-1a
    h := fnv.New64a()
    for _, name := range names {
        h.Write([]byte(name))
        h.Write([]byte{0})  // separator
        h.Write([]byte(s.Labels[name]))
        h.Write([]byte{0})  // separator
    }

    return h.Sum64()
}
```

### MemTable

The `MemTable` is an in-memory buffer for incoming samples:

```go
type MemTable struct {
    series     map[uint64][]Sample     // seriesHash -> samples
    seriesMeta map[uint64]*Series      // seriesHash -> metadata
    size       int64                   // bytes used
    maxSize    int64                   // threshold for flush
    minTime    int64                   // oldest sample
    maxTime    int64                   // newest sample
    mu         sync.RWMutex            // concurrency control
}
```

**Design Decisions:**

1. **Hash-based lookup**: O(1) series lookup using pre-computed hash
2. **Separate metadata storage**: Series labels stored once, not duplicated per sample
3. **Size tracking**: Approximate memory usage for flush triggering
4. **Time range tracking**: Enable quick time-range filtering
5. **RWMutex**: Allow concurrent reads, exclusive writes

**MemTable Operations:**

- **Insert**: O(1) amortized append to sample slice
- **Query**: O(n) scan of samples (will be optimized in later phases)
- **Thread-safety**: All operations protected by RWMutex

**Memory Estimation:**

```
Per Sample:     16 bytes (timestamp + value)
Per Series:     ~100-500 bytes (labels + hash + overhead)
Default limit:  256 MB
Capacity:       ~16M samples or ~500K series (approximate)
```

### Concurrency Model

**Phase 1 Concurrency:**

- **RWMutex**: Simple read-write lock for MemTable
- **Multiple readers**: Concurrent queries allowed
- **Single writer**: Inserts are serialized (sufficient for Phase 1)

**Future Improvements (Phase 2+):**

- Lock-free data structures for hot path
- Per-series sharding to reduce lock contention
- Double-buffering for flushing without blocking writes

## Design Decisions

### Why Go?

1. **Performance**: Native compilation, efficient runtime
2. **Concurrency**: Built-in goroutines and channels
3. **Ecosystem**: Excellent libraries (Prometheus, gRPC, etc.)
4. **Memory safety**: Garbage collection without manual management
5. **Tooling**: Great profiling, testing, and benchmarking tools

### Why FNV-1a Hash?

Alternatives considered:

| Algorithm | Speed      | Distribution | Crypto | Decision |
|-----------|------------|--------------|--------|----------|
| FNV-1a    | Very fast  | Good         | No     | âœ“ Chosen |
| xxHash    | Fastest    | Excellent    | No     | Overkill |
| CityHash  | Fast       | Excellent    | No     | Complex  |
| SHA-256   | Slow       | Excellent    | Yes    | Too slow |

**Rationale**: FNV-1a provides excellent performance-to-complexity ratio. Cryptographic properties aren't needed for in-memory hash tables.

### Why Map-Based MemTable?

Alternatives considered:

| Approach      | Insert | Query  | Memory | Complexity |
|---------------|--------|--------|--------|------------|
| Map + Slice   | O(1)   | O(n)   | Medium | Low        | âœ“ Chosen
| B-Tree        | O(log) | O(log) | High   | High       |
| Skip List     | O(log) | O(log) | Medium | Medium     |
| Array         | O(n)   | O(n)   | Low    | Low        |

**Rationale**: For Phase 1, map-based approach is simple and performs well. Future phases can optimize queries with indexing.

### Why Approximate Size Tracking?

Exact memory tracking would require:
- Profiling every allocation
- Accounting for Go runtime overhead
- Significant performance cost

Approximate tracking:
- Fast: O(1) increment
- Good enough: Within 10-20% accuracy
- Simple: Easy to understand and maintain

### Why 256MB Default MemTable Size?

**Considerations:**

- **Too small**: Frequent flushes, high I/O overhead
- **Too large**: Long recovery time, high memory usage
- **Sweet spot**: 256MB provides:
  - ~16M samples buffer
  - Flush every 1-3 minutes at 100K writes/sec
  - Reasonable crash recovery time
  - Fits in L3 cache for hot data

Configurable for different workloads.

## Performance Considerations

### Phase 1 Performance

**Measured Performance:**

- Series hashing: 1M+ hashes/second
- MemTable inserts: 100K+ inserts/second
- MemTable queries: 500K+ queries/second
- Concurrent scaling: Near-linear with cores

**Bottlenecks:**

1. âœ“ Series hashing: Optimized with FNV-1a
2. âœ“ Lock contention: Mitigated with RWMutex
3. ðŸš§ Query scans: Will be optimized in Phase 4 with indexing
4. ðŸš§ Memory allocation: Will be optimized with object pooling

### Memory Layout

**Cache Efficiency:**

- Series hash: 8 bytes (cache-friendly lookup)
- Samples: Contiguous slice (good cache locality)
- Labels: Map overhead acceptable for infrequent access

**Memory Overhead:**

```
Sample:         16 bytes (data)
Slice overhead: ~24 bytes (pointer + len + cap)
Map entry:      ~16 bytes (key + value pointer)
Total/sample:   ~32 bytes (2x data size)
```

This is acceptable for in-memory buffer. Compression (Phase 3) will reduce to <2 bytes/sample on disk.

### Concurrency Performance

**Read Scaling:**

- RWMutex allows multiple concurrent readers
- Queries scale linearly with CPU cores
- No contention for read-only operations

**Write Scaling:**

- Phase 1: Single writer (simple, sufficient)
- Phase 2: Will add double-buffering
- Phase 3: Will add per-series sharding

## Future Phases

### Phase 2: Write-Ahead Log (WAL)

**Design:**

```
WAL Entry Format:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Length â”‚ Checksum â”‚   Data     â”‚  CRC32  â”‚
â”‚ 4B     â”‚ 8B       â”‚   N bytes  â”‚  4B     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Operations:**

1. Write to WAL (durability)
2. Write to MemTable (performance)
3. Rotate WAL segments (128MB)
4. Truncate after flush (cleanup)

### Phase 3: Block Storage

**Design:**

```
Block Structure:
data/
â””â”€â”€ 01H8XABC00000000/        # Block ULID
    â”œâ”€â”€ meta.json            # Metadata
    â”œâ”€â”€ index                # Inverted index
    â”œâ”€â”€ chunks/              # Compressed data
    â”‚   â”œâ”€â”€ 000001
    â”‚   â””â”€â”€ 000002
    â””â”€â”€ tombstones           # Deletions
```

**Compression:**

- Delta-of-delta for timestamps (Gorilla)
- XOR encoding for values (Gorilla)
- Target: 12 bytes/sample â†’ 1.5 bytes/sample (8x)

### Phase 4: Inverted Index

**Design:**

```
Index Structure:
labelName -> labelValue -> PostingList (bitmap of series IDs)

Example:
"host" -> "server1" -> [1, 5, 42, 100, ...]
"host" -> "server2" -> [2, 6, 43, 101, ...]
```

**Query Optimization:**

- Bitmap intersection for AND queries
- Bitmap union for OR queries
- O(log n) label value lookup

### Phase 5: Query Engine

**Features:**

- Time-range queries
- Aggregations (sum, avg, max, min, count)
- Rate calculations
- Downsampling
- Group-by labels

### Phase 6: Background Operations

**Compaction:**

```
Level 0: 2-hour blocks (raw)
Level 1: 12-hour blocks (merge 6x L0)
Level 2: 7-day blocks (merge 14x L1)
```

**Retention:**

- Automatic deletion of old blocks
- Configurable retention period
- Tombstone-based series deletion

## Testing Strategy

### Unit Tests

- **Coverage**: 80%+ for all packages
- **Concurrency**: Race detector enabled
- **Edge cases**: Empty data, overflow, errors

### Benchmarks

- **Series hashing**: 1M hashes/second target
- **MemTable ops**: 100K inserts/second target
- **Concurrent**: Parallel benchmarks for scaling

### Integration Tests

- End-to-end write â†’ read paths
- Crash recovery scenarios
- Performance stress tests

## Conclusion

Phase 1 establishes a solid foundation with:

- âœ“ Efficient core data structures
- âœ“ Thread-safe in-memory buffer
- âœ“ Fast series identification
- âœ“ Comprehensive test coverage
- âœ“ Performance benchmarking

The design is extensible for future phases while maintaining simplicity and performance for current functionality.

---

**Document Version**: 1.0
**Last Updated**: Phase 1 Implementation
**Status**: Complete âœ“
